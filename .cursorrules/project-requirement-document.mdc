---
description: 
globs: 
alwaysApply: false
---
Excellent ‚Äî your document has a very solid structure, but as you correctly mentioned, it will benefit a lot from refinement for AI coding agents (Cursor, Devin, etc.).
These agents work better when the requirements are:
	‚Ä¢	Deterministic
	‚Ä¢	Explicit in data flows
	‚Ä¢	Clearly modularized into components/services
	‚Ä¢	Properly annotated where human judgment is allowed
	‚Ä¢	Have minimal ambiguity

Let me now refine your entire Project Requirements Document into AI Agent-Friendly format:

‚∏ª

üìå Project Requirements Document: Pickwise

üìù Project Overview

Build an AI-powered Resume Screening System leveraging RAG (Retrieval Augmented Generation), LangChain, and LangGraph that ingests resumes, processes them into structured JSON data, evaluates them against job descriptions, company values, and custom heuristics, ranks candidates, and generates screening questions.

‚∏ª

1Ô∏è‚É£ Functional User Stories

‚úÖ Core Functionalities
	‚Ä¢	As HR, I want to automatically highlight standout resumes.
	‚Ä¢	As HR, I want top-N resumes filtered based on multiple job-specific and company-specific criteria.
	‚Ä¢	As HR, I want resumes categorized into 4 buckets: Top, Go-After, Consider, Sink.
	‚Ä¢	As HR, I want each candidate to be scored and ranked based on:
	‚Ä¢	Job Description match
	‚Ä¢	Past success alignment
	‚Ä¢	Company core values alignment
	‚Ä¢	Uniqueness & standout factors
	‚Ä¢	Achievements
	‚Ä¢	As HR, I want system-generated dynamic screening questions targeting candidate-provided resume content.
	‚Ä¢	As HR, I want ability to compare two resumes side-by-side with system-generated summaries & insights.

‚∏ª

2Ô∏è‚É£ System Architecture & Components

A. Resume Data Extraction Service (Pre-Processor Agent)

üìå Purpose:

Extract structured fields from uploaded resume PDFs into a deterministic JSON format.

üìå Input:
	‚Ä¢	Resume PDFs (stored in S3)

üìå Output:
	‚Ä¢	Extracted JSON files (stored back into S3)

üìå Process:
	‚Ä¢	Process PDFs in parallel batches.
	‚Ä¢	Use OpenAI GPT-4o API with structured prompts and predefined JSON schema.
	‚Ä¢	Design idempotent extraction to ensure reproducibility.

üìå Example JSON Schema (for agents to follow):

{
  "full_name": "",
  "contact_details": {
    "email": "",
    "phone": "",
    "linkedin": ""
  },
  "education": [
    {"degree": "", "institution": "", "year": ""}
  ],
  "work_experience": [
    {"company": "", "role": "", "duration": "", "key_achievements": ""}
  ],
  "skills": [],
  "certifications": [],
  "awards": [],
  "projects": [],
  "publications": [],
  "summary": "",
  "languages": []
}

üìå Notes for AI Agents:
	‚Ä¢	Use strict key mapping.
	‚Ä¢	Fuzzy entity extraction allowed but store confidence scores if applicable.
	‚Ä¢	Always log parsing errors for non-standard resumes.

‚∏ª

B. Resume Evaluation Multi-Agent Service (Main LangGraph Pipeline)

üìå Exposure:
	‚Ä¢	API Endpoint for orchestration of evaluation pipeline.

üìå Input Payload:

{
  "resume_url": "<S3-JSON URL>",
  "job_description_url": "<S3-JD File URL>",
  "company_values_url": "<S3-CompanyValues File URL>",
  "example_resume_insights_output_template_url": "<S3-Template URL>",
  "past_successes_insight_document": "<S3-PastSuccessDoc URL>"
}

üìå Pipeline Nodes (LangGraph Nodes)

‚∏ª

Node 1: JD Analysis Agent
	‚Ä¢	Input: Job Description File
	‚Ä¢	Output: JD alignment score (0-10), alignment justification
	‚Ä¢	Logic:
	‚Ä¢	Identify mandatory vs nice-to-have skills.
	‚Ä¢	Extract weights for each skill.
	‚Ä¢	Provide score breakdown.

LangChain Thought Process Prompt Tip
	‚Ä¢	"Think step-by-step. First extract skill list. Then assign weights. Then compute alignment factor."

‚∏ª

Node 2: Routing Decision
	‚Ä¢	Logic:
	‚Ä¢	If JD alignment score below threshold (e.g. < 5), mark as REJECTED.
	‚Ä¢	Else forward to next nodes.
	‚Ä¢	Output: Either

{ "verdict": "false", "reason": "<Explain why>" }

or continue pipeline.

‚∏ª

Node 3: Company Values + Cultural Fit Agent
	‚Ä¢	Input: Company values file + Resume data
	‚Ä¢	Output: Cultural fit score (0-10), justification text.
	‚Ä¢	Logic:
	‚Ä¢	Match behavioral signals, soft skills, value alignment.

Uniqueness & Custom Criteria Agent
	‚Ä¢	Input: Resume data
	‚Ä¢	Output: Uniqueness score (0-10), justification text.
	‚Ä¢	Logic:
	‚Ä¢	Detect standout achievements, unique skills, rare combinations.

‚∏ª

Node 4: Absolute Rating Computation
	‚Ä¢	Input: Scores from Node 1, Node 3.
	‚Ä¢	Logic:
	‚Ä¢	Calculate weighted composite score.
	‚Ä¢	Suggested weightage (can be parameterized):
	‚Ä¢	JD Score: 50%
	‚Ä¢	Cultural Fit: 30%
	‚Ä¢	Uniqueness: 20%
	‚Ä¢	Output:

{
  "final_score": 7.6,
  "verdict": "PASS",
  "boundary_case_flag": true
}


‚∏ª

Node 6: Relative Ranking (Across Resumes)
	‚Ä¢	Logic:
	‚Ä¢	Compare all candidate scores.
	‚Ä¢	Produce relative ranking table.
	‚Ä¢	Categorize into:
	‚Ä¢	Top
	‚Ä¢	Go-After
	‚Ä¢	Consider
	‚Ä¢	Sink
	‚Ä¢	Output: Tabular output with ranks and categories.

‚∏ª

Node 7: Screening Question Generator
	‚Ä¢	Input: Candidate's extracted data.
	‚Ä¢	Output: List of custom screening questions.
	‚Ä¢	Logic:
	‚Ä¢	Identify areas needing clarification.
	‚Ä¢	Generate 3-5 second-order questions.

‚∏ª

3Ô∏è‚É£ Design Principles
	‚Ä¢	‚úÖ Deterministic scoring ‚Äî avoid random fluctuation on repeated inputs.
	‚Ä¢	‚úÖ Modular chain-of-thought prompts for higher stability.
	‚Ä¢	‚úÖ Fully auditable decision-making with justification logs.
	‚Ä¢	‚úÖ HR boundary-case override capability retained.

‚∏ª

4Ô∏è‚É£ Tech Stack Recommendations

Component	Technology
LLM Calls	OpenAI GPT-4o
LangChain	LangGraph Orchestration
Storage	AWS S3
Parallel Processing	AWS Lambda + S3 Triggers
Backend	FastAPI / Node.js
UI	React + Tailwind


‚∏ª

5Ô∏è‚É£ Engineering Tasks

üîß Resume Data Extraction Service
	‚Ä¢	Batch processing of uploaded PDFs.
	‚Ä¢	Use OpenAI GPT-4o + extraction prompt ‚Üí JSON schema.
	‚Ä¢	Store output in S3 (with versioning).
	‚Ä¢	Parallelize using batching/Lambda triggers.

üîß Build LangGraph Multi-Agent Pipeline (MVP)
	‚Ä¢	Setup nodes as per design above.
	‚Ä¢	Tune prompts for determinism.
	‚Ä¢	Build routing & scoring logic.
	‚Ä¢	Build backend API endpoints to trigger full evaluation chain.

üîß Develop UI & Integrations
	‚Ä¢	Display:
	‚Ä¢	Resume scores
	‚Ä¢	Rankings table
	‚Ä¢	Category buckets
	‚Ä¢	Screening questions
	‚Ä¢	Resume comparison tool

‚∏ª

6Ô∏è‚É£ Prompt Engineering Notes (for LLM Agents)
	‚Ä¢	Use strict thought decomposition:
1Ô∏è‚É£ Extract ‚Üí 2Ô∏è‚É£ Reason ‚Üí 3Ô∏è‚É£ Score ‚Üí 4Ô∏è‚É£ Justify
	‚Ä¢	Include system instructions like:

"Never guess if uncertain; output 'UNKNOWN' field with low confidence score."

	‚Ä¢	Build reusable prompt templates for each evaluation node.

‚∏ª

7Ô∏è‚É£ Non-Functional Considerations
	‚Ä¢	Reproducibility of scoring
	‚Ä¢	Prompt versioning control
	‚Ä¢	API latency considerations
	‚Ä¢	Resume template edge case handling
	‚Ä¢	Logging of each scoring sub-decision for audits

